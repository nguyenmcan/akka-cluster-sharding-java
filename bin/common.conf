# In this file you can override any option defined in the reference files.
# Copy in parts of the reference files and modify as you please.

worker-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}

dedicator-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}


router-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}

producer-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}

akka {

  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.Logging$DefaultLogger"]
 
  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "OFF"
 
  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "INFO"
 
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
 
    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 10
    }
    
	deployment {
		/router1 {
			router = consistent-hashing-pool
			nr-of-instances = 100
			cluster {
				enabled = on
				max-nr-of-instances-per-node = 10
				allow-local-routees = off
				use-role = ""
			}
		}
	}    
    
  }
  
  remote {
    # The port clients should connect to. Default is 2552.
    netty.tcp {
    	port = 2551
    	hostname = "127.0.0.1"
    }
  }
    
  cluster {
    seed-nodes = ["akka.tcp://ActorSystem@127.0.0.1:2551", "akka.tcp://ActorSystem@127.0.0.1:2552"]
 
    failure-detector.heartbeat-interval = 1 s
    auto-down-unreachable-after = 5s
  }
  
  persistence {
    #journal.plugin = "akka.persistence.journal.leveldb-shared"
    #journal.leveldb-shared.store {
    #  native = off
    #  dir = "target/shared-journal"
    #}
    #snapshot-store.local.dir = "target/snapshots"
    journal.plugin = "akka.persistence.journal.leveldb"
    journal.leveldb {
      native = off
      dir = "target/journal"
    }
    snapshot-store.local.dir = "target/snapshots"    
  }
  
  contrib.cluster.sharding {
    # The extension creates a top level actor with this name in top level user scope,
    # e.g. '/user/sharding'
    guardian-name = sharding
    # If the coordinator can't store state changes it will be stopped
    # and started again after this duration.
    coordinator-failure-backoff = 10 s
    # Start the coordinator singleton manager on members tagged with this role.
    # All members are used if undefined or empty.
    # ShardRegion actor is started in proxy only mode on nodes that are not tagged
    # with this role.
    role = ""
    # The ShardRegion retries registration and shard location requests to the 
    # ShardCoordinator with this interval if it does not reply.
    retry-interval = 2 s
    # Maximum number of messages that are buffered by a ShardRegion actor.
    buffer-size = 100000
    # Timeout of the shard rebalancing process.
    handoff-timeout = 60 s
    # Time given to a region to acknowdge it's hosting a shard.
    shard-start-timeout = 10 s
    # If the shard can't store state changes it will retry the action
    # again after this duration. Any messages sent to an affected entry
    # will be buffered until the state change is processed
    shard-failure-backoff = 10 s
    # If the shard is remembering entries and an entry stops itself without
    # using passivate. The entry will be restarted after this duration or when
    # the next message for it is received, which ever occurs first.
    entry-restart-backoff = 10 s
    # Rebalance check is performed periodically with this interval.
    rebalance-interval = 5 s
    # How often the coordinator saves persistent snapshots, which are
    # used to reduce recovery times
    snapshot-interval = 3600 s
    # Setting for the default shard allocation strategy
    least-shard-allocation-strategy {
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    rebalance-threshold = 1 
    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }
  }  
  
  log-dead-letters = off
  log-dead-letters-during-shutdown = off
  jvm-exit-on-fatal-error = off
 
  extensions = ["akka.contrib.pattern.ClusterReceptionistExtension", "akka.contrib.pattern.DistributedPubSubExtension", "akka.contrib.pattern.ClusterSharding"]
  # "kamon.logreporter.LogReporter", "kamon.metric.Metrics"

}

# Kamon Metrics
# ~~~~~~~~~~~~~~

kamon {

  metrics {
    filters = [
      {
        actor {
          includes = [ "user/producer-manager/producer/*" ]
          excludes = [ "system/*" ]
        }
      },
      {
        trace {
          includes = [ "*" ]
          excludes = []
        }
      }
    ]
  }
}
